{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "theoretical-dating",
   "metadata": {},
   "source": [
    "### Organize GSR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-omaha",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.anova import AnovaRM, anova_lm\n",
    "from scipy.stats import ttest_rel\n",
    "from scipy.stats.mstats import winsorize\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import jarque_bera, zscore, boxcox\n",
    "from pingouin import pairwise_tests\n",
    "\n",
    "today=str(date.today())\n",
    "pd.set_option('display.max_rows', 999)\n",
    "pd.set_option('display.max_columns', 999)\n",
    "plt.rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsrpath = '/gpfs/milgram/project/gee_dylan/candlab/scripts/shapes/gsr/pspm'\n",
    "datapath = '/gpfs/milgram/pi/gee_dylan/candlab/analyses/shapes/shapes_phenotyping'\n",
    "analysis = datapath + '/Analysis'\n",
    "bv_data = pd.read_csv(analysis + '/Behav_Dataset_n=117_2023-03-08.csv') \n",
    "gsrqa = pd.read_csv(analysis + '/GSRQAShapes-GSRratings_Labels_9.25.23.csv')\n",
    "gsrqa['Subject'] = 'sub-' + gsrqa['Subject ID'].str.replace('--1','').str.replace('--2','').str.replace('-P','')\n",
    "sublist = bv_data['Subject'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts for data read in\n",
    "print(len(gsrqa['Subject']), 'subjects included')\n",
    "print(len(list(set(gsrqa['Subject']))), 'subjects (de-duplicated)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify subjects with multiple ratings\n",
    "problem_qa = []\n",
    "\n",
    "for i in range(0, len(list(set(gsrqa['Subject'])))):\n",
    "    subj = list(set(gsrqa['Subject']))[i] # select ID\n",
    "    dset = gsrqa[gsrqa['Subject'] == subj] # get dataset with just that ID number (all QA entries)\n",
    "    if len(list(set(dset['Overall GSR QA Rating']))) > 1:\n",
    "        problem_qa.append(subj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get count of number of QA entries for each subject\n",
    "num_entries = pd.DataFrame(gsrqa['Subject'].value_counts()).reset_index().rename(columns = {'Subject':'Count',\n",
    "                                                                                            'index':'Subject'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter entries and keep only one, ideally the double-entered version. Some IDs have multiple entries (greater than 2)\n",
    "subj_ids_filtering = []\n",
    "for i in range(0, len(list(set(gsrqa['Subject'])))):\n",
    "    subj = list(set(gsrqa['Subject']))[i] # Select subject\n",
    "    sub_dset = gsrqa[gsrqa['Subject'] == subj].reset_index(drop=True) # get df with all QA ratings for that subject\n",
    "    entries = num_entries[num_entries['Subject'] == subj] # see how many entries that subject has\n",
    "    \n",
    "    # If subject has only one rating, save that rating\n",
    "    if entries['Count'].item() == 1:\n",
    "        subj_ids_filtering.append(sub_dset['Subject ID'][0])\n",
    "        \n",
    "    # If subject has more than one rating...\n",
    "    else:\n",
    "        if len(list(set(sub_dset['Overall GSR QA Rating']))) == 1: # if there is only a single 'overall' rating, save that\n",
    "            subj_ids_filtering.append(sub_dset['Subject ID'][0])\n",
    "\n",
    "        else:\n",
    "            if sub_dset['Subject ID'][0] == subj.lstrip('sub-'): # If subject was mis-entered without sub- attached, save their record\n",
    "                subj_ids_filtering.append(sub_dset['Subject ID'][0])\n",
    "\n",
    "            else: # Otherwise follow up manually\n",
    "                print('ERROR on ', subj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through error subjects manually and choose which records to keep\n",
    "manual_updates = ['A992--1', 'A661--1', 'A556--2'] # keep these\n",
    "subj_ids_fjoined = subj_ids_filtering + manual_updates\n",
    "\n",
    "assert len(subj_ids_fjoined) == 178"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to select subjects\n",
    "gsr_qa_dset = pd.merge(gsrqa, pd.DataFrame(subj_ids_fjoined, columns = ['Subject ID']), how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasgsrlist = glob(gsrpath + '/*_run3*_6.txt')\n",
    "\n",
    "hasgsr = []\n",
    "for i in range(0, len(hasgsrlist)):\n",
    "    line = hasgsrlist[i]\n",
    "    subid = line.replace('/gpfs/milgram/project/gee_dylan/candlab/scripts/shapes/gsr/pspm/', '').split('_')[0]\n",
    "    hasgsr.append(subid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate IDs\n",
    "hasgsr = list(set(hasgsr))\n",
    "print(hasgsr[0:5], '..., n =', len(hasgsr))\n",
    "print(len(hasgsr), 'subjects have GSR QA ratings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_gsr = pd.DataFrame(hasgsr, columns = ['Subject'])\n",
    "subs_gsr['Subject'] = 'sub-' + subs_gsr['Subject']\n",
    "subs_gsr['HasGSR'] = 1\n",
    "\n",
    "comb_df1 = pd.merge(bv_data['Subject'], subs_gsr, on='Subject', how='outer').sort_values(by='HasGSR', ascending=True)\n",
    "comb_df = pd.merge(comb_df1, gsr_qa_dset, on='Subject', how='left')\n",
    "hasgsrdf = comb_df[comb_df['HasGSR'] == 1]\n",
    "usablegsrdf = hasgsrdf[(hasgsrdf['Overall GSR QA Rating'] == 'Pass')].dropna(how='any', axis=0) #| (hasgsrdf['Overall GSR QA Rating'] == 'Qualified Pass')\n",
    "\n",
    "newsubsgsr = usablegsrdf['Subject'].tolist()\n",
    "print('{} subject are missing GSR data'.format(len(comb_df[comb_df['HasGSR'] != 1])))\n",
    "print('{} out of {} subjects have usable data'.format(len(usablegsrdf), len(hasgsrdf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-trader",
   "metadata": {},
   "source": [
    "**Inclusion:**\n",
    "43 subjects if include just pass,\n",
    "59 subjects if include pass & qualified pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-greensboro",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gsr_datatrain = np.ones((len(newsubsgsr), 6))\n",
    "gsr_data1 = np.ones((len(newsubsgsr), 6))\n",
    "gsr_data2 = np.ones((len(newsubsgsr), 6))\n",
    "\n",
    "for i in range(0, len(newsubsgsr)):\n",
    "    sub = newsubsgsr[i]\n",
    "    \n",
    "    runtrain = pd.read_csv(gsrpath + '/{}_run1_earlylate_stats_case2_6.txt'.format(sub.lstrip('sub-')), sep = '\\t', header = 1)\n",
    "    assert runtrain.columns[2] == 'Stimulus_Aminus_early recon'\n",
    "    assert runtrain.columns[3] == 'Stimulus_Aminus_late recon'\n",
    "    \n",
    "    run1 = pd.read_csv(gsrpath + '/{}_run2_stats_case2_6.txt'.format(sub.lstrip('sub-')), sep = '\\t', header = 1)\n",
    "    assert run1.columns[1] == 'Stimulus_Aminus recon'\n",
    "    assert run1.columns[2] == 'Stimulus_Bminus recon'\n",
    "    \n",
    "    run2 = pd.read_csv(gsrpath + '/{}_run3_stats_case2_6.txt'.format(sub.lstrip('sub-')), sep = '\\t', header = 1)\n",
    "    assert run2.columns[1] == 'Stimulus_Aminus recon'\n",
    "    assert run2.columns[2] == 'Stimulus_Bminus recon'\n",
    "\n",
    "    means = pd.concat([run1, run2], axis=0).mean(axis=0)\n",
    "    gsr_datatrain[i, :] = runtrain.iloc[:,0:6] #Drop last column of NaNs\n",
    "    gsr_data1[i, :] = run1.iloc[:,0:10]\n",
    "    gsr_data2[i, :] = run2.iloc[:,0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize results\n",
    "\n",
    "# Training Run\n",
    "gsr_traindf = pd.DataFrame(gsr_datatrain, columns = runtrain.columns[0:6], index=newsubsgsr).reset_index().dropna(how='all', axis=1).rename(columns = {'index':'Subject'})\n",
    "gsr_traindf['Run'] = 'Training' # Assign name to run\n",
    "\n",
    "# Testing Run 1\n",
    "gsr_df1 = pd.DataFrame(gsr_data1, columns = run1.columns[0:10], index = newsubsgsr).reset_index().dropna(how='all', axis=1).rename(columns = {'index':'Subject'})\n",
    "gsr_df1['Run'] = 'Run1'\n",
    "\n",
    "# Testing Run 2\n",
    "gsr_df2 = pd.DataFrame(gsr_data2, columns = run2.columns[0:10], index = newsubsgsr).reset_index().dropna(how='all', axis=1).rename(columns = {'index':'Subject'})\n",
    "gsr_df2['Run'] = 'Run2'\n",
    "gsr_df = pd.concat([gsr_df1, gsr_df2], axis=0).rename(columns = {'Stimulus_Aminus_early recon':'Early_Threat',\n",
    "                                                                'Stimulus_Bminus_early recon':'Early_Safety',\n",
    "                                                                'Stimulus_Aminus_late recon':'Late_Threat',\n",
    "                                                                'Stimulus_Bminus_late recon':'Late_Safety',\n",
    "                                                                'Stimulus_Aminus recon':'Threat',\n",
    "                                                                'Stimulus_Bminus recon':'Safety'})\n",
    "\n",
    "training_df = gsr_traindf.rename(columns = {'Stimulus_Aplus_early recon':'Early_Threat_Reinforced',\n",
    "                                            'Stimulus_Aminus_early recon':'Early_Threat',\n",
    "                                            'Stimulus_Bminus_early recon':'Early_Safety',\n",
    "                                            'Stimulus_Aplus_late recon':'Late_Threat_Reinforced',\n",
    "                                           'Stimulus_Aminus_late recon':'Late_Threat',\n",
    "                                           'Stimulus_Bminus_late recon':'Late_Safety'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt and reshape data to long format\n",
    "reshaped_training = pd.melt(training_df, \n",
    "                            value_vars = ['Early_Threat', 'Early_Safety',\n",
    "                                          'Late_Threat', 'Late_Safety'], \n",
    "                      id_vars = ['Subject', 'Run'], \n",
    "                      var_name = 'Task Condition',\n",
    "                     value_name='Reconstructed Response Value').sort_values(by='Subject')\n",
    "reshaped_training['Reconstructed Response Value'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-measure",
   "metadata": {},
   "source": [
    "### Check Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-pasta",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers(data, column):\n",
    "    outliers = []\n",
    "    val_mean = data[column].mean()\n",
    "    val_std = data[column].std()\n",
    "    for i in range(0, len(data)):\n",
    "        dset = data.iloc[i, :]\n",
    "        if dset[column] > (val_mean + 3*val_std): # If value more than 3 SD from mean\n",
    "            outliers.append(dset['Subject'])\n",
    "        elif dset[column] < (val_mean - 3*val_std): # If value less than 3 SD from mean\n",
    "            outliers.append(dset['Subject'])\n",
    "    print(len(outliers), 'outliers were found')\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329872dc-c3a8-4ec6-86be-8a2ceed703c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(15,4))\n",
    "\n",
    "sns.histplot(training_df['Early_Threat'], ax = ax1)\n",
    "sns.histplot(training_df['Early_Safety'], ax = ax2)\n",
    "sns.histplot(training_df['Late_Threat'], ax = ax3)\n",
    "sns.histplot(training_df['Late_Safety'], ax = ax4)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and remove outliers by condition and run\n",
    "i_outliers = find_outliers(training_df, 'Early_Threat')\n",
    "j_outliers = find_outliers(training_df, 'Early_Safety')\n",
    "k_outliers = find_outliers(training_df, 'Late_Threat')\n",
    "l_outliers = find_outliers(training_df, 'Late_Safety')\n",
    "\n",
    "total_outliers = list(set(i_outliers + j_outliers + k_outliers + l_outliers))\n",
    "print(len(total_outliers), 'total outliers')\n",
    "print(total_outliers)\n",
    "\n",
    "# #Winsorise outliders\n",
    "# training_df_wins = deepcopy(training_df)\n",
    "# for i in range(1, 7):\n",
    "#     col = training_df_wins.columns[i]\n",
    "#     print('Column: ', col)\n",
    "#     winsorize(training_df_wins[col], limits= [.1, .1], inclusive=[False, False], inplace=True)\n",
    "\n",
    "# Drop outliers\n",
    "training_clean_df = training_df[training_df.Subject.isin(total_outliers) == False]\n",
    "assert len(training_clean_df) == len(training_df) - len(total_outliers) #Ensure outliers were dropped\n",
    "    \n",
    "print('Stats conducted now with {} subs instead of {}'.format(len(training_clean_df), len(training_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_training = pd.melt(training_clean_df, \n",
    "                            value_vars = ['Early_Threat', 'Early_Safety',\n",
    "                                          'Late_Threat', 'Late_Safety'], \n",
    "                      id_vars = ['Subject', 'Run'], \n",
    "                      var_name = 'Task Condition',\n",
    "                     value_name='Reconstructed Response Value').sort_values(by='Subject')\n",
    "\n",
    "reshaped_training['Timing'] = reshaped_training['Task Condition'].str.split('_', expand=True).iloc[:, 0] + ' Phase'\n",
    "reshaped_training['Condition'] = reshaped_training['Task Condition'].str.split('_', expand=True).iloc[:, 1]\n",
    "\n",
    "# Normalize and factorize variables\n",
    "reshaped_training['Timing'] = reshaped_training['Timing'].astype('category')\n",
    "reshaped_training['Condition'] = reshaped_training['Condition'].astype('category')\n",
    "reshaped_training['Reconstructed Response Value'] = zscore(reshaped_training['Reconstructed Response Value'].astype('float')) #Normalize SCR responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig, (ax2) = plt.subplots(1, 1, figsize = (7, 5))\n",
    "\n",
    "sns.boxplot(x = 'Timing', y = 'Reconstructed Response Value', hue = 'Condition', data = reshaped_training, ax=ax2,\n",
    "            order = ['Early Phase', 'Late Phase'],\n",
    "            palette = dict(Safety='#63a7e6', Threat='red'))\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim(-3, 6)\n",
    "ax2.set_xlabel('Acquisition Run')\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis + \"/Figures/AcquisitionPhase_GSRPlots_{}.png\".format(today), dpi=300, transparent=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing data to prepare for mixed effects modeling\n",
    "\n",
    "t_df = pd.merge(reshaped_training, bv_data, on='Subject', how='inner').dropna(subset = ['asr_age',\n",
    "                                                                                 'sex',\n",
    "                                                                                 'combined_income',\n",
    "                                                                                 'years_education',\n",
    "                                                                                 'diagnostic_group']).reset_index()\n",
    "print('Analysis conducted with {} subjects, {} dropped due to missing data'.format(len(t_df['Subject'].value_counts()), len(reshaped_training['Subject'].value_counts()) - len(t_df['Subject'].value_counts())))\n",
    "\n",
    "# Normalize and factorize variables\n",
    "t_df['task_condition'] = t_df['Task Condition'].astype('category')\n",
    "t_df['sex'] = t_df['sex'].astype('category')\n",
    "t_df['combined_income'] = t_df['combined_income'].astype('category')\n",
    "t_df['years_education'] = t_df['years_education'].astype('category')\n",
    "t_df['diagnostic_group'] = t_df['diagnostic_group'].astype('category')\n",
    "t_df['asr_age'] = zscore(t_df['asr_age'].astype('float'))\n",
    "t_df['reconstructed_value']= t_df['Reconstructed Response Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit mixed effects models\n",
    "\n",
    "# Plot distribution\n",
    "sns.histplot(t_df['reconstructed_value'])\n",
    "plt.show()\n",
    "\n",
    "#Omnibus model\n",
    "mod_t = smf.mixedlm(\"reconstructed_value ~ Condition + Timing +asr_age + sex + combined_income + years_education\", \n",
    "                groups=\"Subject\", data= t_df);\n",
    "t_results = mod_t.fit();\n",
    "print(t_results.summary())\n",
    "\n",
    "# Pairwise tests\n",
    "pairwise_tests(data = t_df, dv = 'reconstructed_value', within = 'Timing', between = 'Condition', subject = 'Subject',\n",
    "              parametric = True, marginal = True, padjust = 'fdr_bh', effsize = 'cohen', return_desc=True).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-firmware",
   "metadata": {},
   "source": [
    "### Check Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f35eb4-a5d8-4b0c-9b37-59c5d5aeba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,4))\n",
    "\n",
    "sns.histplot(gsr_df['Threat'], ax = ax1)\n",
    "sns.histplot(gsr_df['Safety'], ax = ax2)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers from Testing data\n",
    "\n",
    "a_outliers = find_outliers(gsr_df, 'Threat')\n",
    "b_outliers = find_outliers(gsr_df, 'Safety')\n",
    "\n",
    "total_outliers_test = list(set(a_outliers + b_outliers)) \n",
    "print(len(total_outliers_test), 'total outliers out of', len(gsr_df), 'subjects')\n",
    "total_outliers_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Winsorise outliders\n",
    "# gsr_df_wins = deepcopy(gsr_df)\n",
    "# for col in ['Threat', 'Safety']:\n",
    "#     print('Column: ', col)\n",
    "#     winsorize(gsr_df_wins[col], limits= [.1, .1], inclusive=[False, False], inplace=True)\n",
    "\n",
    "# Drop outliers data[data.marks.isin(list1) == False])\n",
    "gsr_clean_df = gsr_df[gsr_df.Subject.isin(total_outliers_test) == False]\n",
    "print('Testing phase analyses conducted with {} subjects'.format(len(gsr_clean_df)/2))\n",
    "\n",
    "assert len(gsr_clean_df) == len(gsr_df) - len(total_outliers_test)*2 #Assert subs across both runs (*2) are dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbe5495-c275-4b41-9ad5-de6a055f1bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge training and testing together to save\n",
    "train_test_gsr = pd.merge(gsr_clean_df, training_clean_df, on='Subject', how='outer')\n",
    "train_test_gsr.to_csv(analysis + '/GSR_data_{}.csv'.format(today))\n",
    "print(analysis+ '/GSR_data_{}.csv'.format(today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-telling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data frame for analysis\n",
    "\n",
    "reshaped_df = pd.melt(gsr_clean_df, value_vars = ['Threat', 'Safety'], \n",
    "                      id_vars = ['Subject', 'Run'], \n",
    "                      var_name = 'Task Condition',\n",
    "                     value_name='Reconstructed Response Value').sort_values(by='Subject')\n",
    "\n",
    "reshaped_df['Task_Condition'] = reshaped_df['Task Condition'].astype('category')\n",
    "reshaped_df['Reconstructed Response Value'] = zscore(reshaped_df['Reconstructed Response Value'].astype('float'))\n",
    "reshaped_df['reconstructed_value'] = zscore(reshaped_df['Reconstructed Response Value'].astype('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "\n",
    "fig, (ax2) = plt.subplots(1, 1, figsize = (7, 5))\n",
    "\n",
    "reshaped_df['Run'] = reshaped_df['Run'].str.replace('Run1', 'First Run').str.replace('Run2', 'Second Run')\n",
    "sns.boxplot(x = 'Run', y = 'Reconstructed Response Value', hue = 'Task Condition', data = reshaped_df, ax=ax2,\n",
    "            order = ['First Run', 'Second Run'], \n",
    "            palette = dict(Safety='#63a7e6', Threat='red'))\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim(-3, 6)\n",
    "ax2.set_xlabel('Testing Runs')\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis + \"/Figures/TestingPhase_GSRPlot_{}.png\".format(today), dpi=300, transparent=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing data to prepare for mixed effects modeling\n",
    "\n",
    "m_df = pd.merge(reshaped_df, bv_data, on='Subject', how='inner').dropna(subset = ['asr_age',\n",
    "                                                                                 'sex',\n",
    "                                                                                 'combined_income',\n",
    "                                                                                 'years_education']).reset_index()\n",
    "m_df['task_condition'] = m_df['Task Condition']\n",
    "print('Analysis conducted with {} subjects, {} dropped due to missing data'.format(len(m_df['Subject'].value_counts()), len(reshaped_training['Subject'].value_counts()) - len(m_df['Subject'].value_counts())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-depression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit mixed effects models\n",
    "\n",
    "# Omnibus model\n",
    "mod = sm.MixedLM.from_formula(\"reconstructed_value ~ Task_Condition + Run + asr_age + sex + combined_income + years_education\", \n",
    "                groups=\"Subject\", data= m_df);\n",
    "aresults = mod.fit();\n",
    "print(aresults.summary())\n",
    "\n",
    "# Pairwise tests\n",
    "pairwise_tests(data = m_df, dv = 'reconstructed_value', within = 'Run', between = 'Task_Condition', subject = 'Subject',\n",
    "              parametric = True, marginal = True, padjust = 'fdr_bh', effsize = 'cohen', return_desc=True).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2b7a48-5d99-4330-a816-b755ab0f65e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
